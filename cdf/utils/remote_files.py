import re
import os
import tempfile
import json

from cdf.analysis.urls.utils import get_part_id
from cdf.exceptions import MalformedFileNameError
from cdf.utils import s3, path
from cdf.core import constants


def enumerate_partitions(uri, only_crawled_urls=False):
    """
    Return the list of partition ids
    :param uri: the crawl data location (s3_uri)
    :type uri: str
    :param only_crawled_urls : Return partitions with at least 1 url crawled inside
    :type only_crawled_urls : boolean
    :returns: list - sorted list of urlids
    """
    regexp = r'urlids\.txt\.([0-9]+)\.gz'
    if s3.is_s3_uri(uri):
        files = s3.list_files(uri, regexp=regexp)
        files = [f.name for f in files]
    else:
        files = path.list_files(uri, regexp=regexp)
    result = [get_part_id_from_filename(f) for f in files]
    result = sorted(result)

    if only_crawled_urls:
        crawl_info = get_crawl_info(uri)
        max_part_id = get_max_crawled_partid(crawl_info)
        return filter(lambda p: p <= max_part_id, result)

    return result


def get_part_id_from_filename(filename):
    """Return the part id from a filename
    If the part id can not be extracted raise a MalformedFileNameError

    :param filename: the input filename
    :type filename: str
    :returns: int -- the part id
    """
    regex = re.compile(".*txt.([\d]+).gz")
    m = regex.match(filename)
    if not m:
        raise MalformedFileNameError(
            "%s does not contained any part id." % filename
        )
    return int(m.group(1))


def get_crawl_info(uri, force_fetch=True):
    """
    Return crawl dictionnary based on JSON data stored on S3
    """
    tmp_dir = tempfile.mkdtemp()
    global_crawl_info_filename = "files.json"
    s3.fetch_file(os.path.join(uri, global_crawl_info_filename),
                  os.path.join(tmp_dir, global_crawl_info_filename),
                  force_fetch=force_fetch)
    crawler_metakeys = load_crawler_metakeys(tmp_dir)
    shutil.rmtree(tmp_dir)
    return crawler_metakeys


def load_crawler_metakeys(data_directory_path):
    """Load the crawler metakeys form file.json
    :param data_directory_path: the path to the directory
                                that contains crawl files
    :type data_directory_path: str
    :returns: dict

    """
    filename = os.path.join(data_directory_path, "files.json")
    with open(filename) as f:
        crawler_metakeys = json.load(f)
    return crawler_metakeys


def get_max_crawled_urlid(crawler_metakeys):
    """Return the highest urlid that has been crawled
    :param crawler_metakeys: the metakeys generated by the crawler
                             usually loaded with load_crawler_metakeys
    :type crawler_metakeys: dict
    :returns: int
    """
    return crawler_metakeys["max_uid_we_crawled"]


def get_max_crawled_partid(crawler_metakeys):
    """Return the highest partid that has been crawled
    :param crawler_metakeys: the metakeys generated by the crawler
                             usually loaded with load_crawler_metakeys
    :type crawler_metakeys: dict
    :returns: int
    """
    first_part_id_size = crawler_metakeys["lines_per_file_first"]
    part_id_size = crawler_metakeys["lines_per_file"]
    return get_part_id(get_max_crawled_urlid(crawler_metakeys),
                       first_part_id_size,
                       part_id_size)
