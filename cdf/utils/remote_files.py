import re
import os
import json

from cdf.analysis.urls.utils import get_part_id
from cdf.exceptions import MalformedFileNameError
from cdf.utils import s3, path
from cdf.tasks.constants import DEFAULT_FORCE_FETCH
from cdf.tasks.decorators import TemporaryDirTask as with_temporary_dir


def enumerate_partitions(uri,
                         first_part_id_size,
                         part_id_size,
                         only_crawled_urls=False):
    """Return the list of partition ids

    :param uri: the crawl data location (s3_uri)
    :type uri: str
    :param first_part_id_size: # urls of the first part
    :type first_part_id_size: int
    :param part_id_size: # urls by part
    :type part_id_size: int
    :param only_crawled_urls: Return partitions with at least 1 url crawled inside
    :type only_crawled_urls: boolean
    :returns: list - sorted list of urlids
    """
    regexp = r'urlids\.txt\.([0-9]+)\.gz'
    if s3.is_s3_uri(uri):
        files = s3.list_files(uri, regexp=regexp)
        files = [f.name for f in files]
    else:
        files = path.list_files(uri, regexp=regexp)
    result = [get_part_id_from_filename(f) for f in files]
    result = sorted(result)

    if only_crawled_urls:
        crawl_info = get_crawl_info(uri)
        max_part_id = get_max_crawled_partid(crawl_info, first_part_id_size, part_id_size)
        return filter(lambda p: p <= max_part_id, result)

    return result


def get_part_id_from_filename(filename):
    """Return the part id from a filename
    If the part id can not be extracted raise a MalformedFileNameError

    :param filename: the input filename
    :type filename: str
    :returns: int -- the part id
    """
    regex = re.compile(".*txt.([\d]+).gz")
    m = regex.match(filename)
    if not m:
        raise MalformedFileNameError(
            "%s does not contained any part id." % filename
        )
    return int(m.group(1))


@with_temporary_dir
def get_crawl_info(uri, tmp_dir=None, force_fetch=DEFAULT_FORCE_FETCH):
    """
    Return crawl dictionnary based on JSON data stored on S3
    """
    filename = "files.json"
    crawl_info_path = os.path.join(tmp_dir, filename)
    s3.fetch_file(os.path.join(uri, filename),
                  crawl_info_path,
                  force_fetch=force_fetch)
    with open(crawl_info_path) as f:
        crawler_metakeys = json.load(f)
    return crawler_metakeys


def load_crawler_metakeys(data_directory_path):
    """Load the crawler metakeys form file.json
    :param data_directory_path: the path to the directory
                                that contains crawl files
    :type data_directory_path: str
    :returns: dict

    """
    filename = os.path.join(data_directory_path, "files.json")
    with open(filename) as f:
        crawler_metakeys = json.load(f)
    return crawler_metakeys


def get_max_crawled_urlid(crawler_metakeys):
    """Return the highest urlid that has been crawled
    :param crawler_metakeys: the metakeys generated by the crawler
                             usually loaded with load_crawler_metakeys
    :type crawler_metakeys: dict
    :returns: int
    """
    return crawler_metakeys["max_uid_we_crawled"]


def get_max_crawled_partid(crawler_metakeys,
                           first_part_id_size,
                           part_id_size):
    """Return the highest partid that has been crawled
    :param crawler_metakeys: the metakeys generated by the crawler
                             usually loaded with load_crawler_metakeys
    :type crawler_metakeys: dict
    :param first_part_id_size : # urls of the first part
    :type first_part_id_size: int
    :param part_id_size : # urls by part
    :type part_id_size: int
    :returns: int
    """
    return get_part_id(get_max_crawled_urlid(crawler_metakeys),
                       first_part_id_size,
                       part_id_size)
