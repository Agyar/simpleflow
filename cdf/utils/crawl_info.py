import tempfile
import os
import json

from cdf.utils.s3 import fetch_file
from cdf.analysis.urls.utils import get_part_id


def get_crawl_info(uri, force_fetch=True):
    """
    Return crawl dictionnary based on JSON data stored on S3
    """
    tmp_dir = tempfile.mkdtemp()
    global_crawl_info_filename = "files.json"
    fetch_file(os.path.join(uri, global_crawl_info_filename),
               os.path.join(tmp_dir, global_crawl_info_filename),
               force_fetch=force_fetch)
    crawler_metakeys = load_crawler_metakeys(tmp_dir)
    shutil.rmtree(tmp_dir)
    return crawler_metakeys


def load_crawler_metakeys(data_directory_path):
    """Load the crawler metakeys form file.json
    :param data_directory_path: the path to the directory
                                that contains crawl files
    :type data_directory_path: str
    :returns: dict

    """
    filename = os.path.join(data_directory_path, "files.json")
    with open(filename) as f:
        crawler_metakeys = json.load(f)
    return crawler_metakeys


def get_max_crawled_urlid(crawler_metakeys):
    """Return the highest urlid that has been crawled
    :param crawler_metakeys: the metakeys generated by the crawler
                             usually loaded with load_crawler_metakeys
    :type crawler_metakeys: dict
    :returns: int

    """
    return crawler_metakeys["max_uid_we_crawled"]


def get_max_crawled_partid(crawler_metakeys):
    first_part_id_size = crawler_metakeys["lines_per_file_first"]
    part_id_size = crawler_metakeys["lines_per_file"]
    return get_part_id(get_max_crawled_urlid(crawler_metakeys),
                       first_part_id_size,
                       part_id_size)
