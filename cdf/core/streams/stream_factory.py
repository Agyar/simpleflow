import os
import re
import gzip
import itertools
import json
import tempfile
import marshal

from urlparse import urlsplit, parse_qs
from cdf.core.streams.utils import split_file

from cdf.log import logger

from cdf.core.features import Feature
from cdf.utils.remote_files import get_part_id_from_filename
from cdf.utils.crawl_info import get_nb_crawled_urls, get_max_crawled_urlid
from cdf.features.main.streams import IdStreamDef, InfosStreamDef
from cdf.features.semantic_metadata.streams import ContentsStreamDef
from cdf.features.semantic_metadata.settings import CONTENT_TYPE_NAME_TO_ID


class StreamFactoryCache(object):
    """A decorator for stream factories that speeds up multiple iterations.
    To achieve this, the decorator reads the stream once from the decorated
    stream factory and serialize it in binary form in a tmp file.
    It makes every pass over the stream much faster.
    """
    def __init__(self, stream_factory, tmp_dir=None):
        """Constructor
        :param stream_factory: the stream factory to decorate
        :param tmp_dir: the tmp directory to use
        :type tmp_dir: str
        """
        if tmp_dir is None:
            f = tempfile.NamedTemporaryFile(delete=False)
        else:
            f = tempfile.NamedTemporaryFile(delete=False, prefix=tmp_dir)
        f.close()
        self.tmp_filename = f.name
        self._cache_stream(stream_factory.get_stream(), self.tmp_filename)

    def __del__(self):
        os.remove(self.tmp_filename)

    def _cache_stream(self, stream, filepath):
        """Cache the stream in a file
        :param stream: the stream to cache
        :type stream: iterator
        :param filepath: the path to the file where to dump the stream
        :type filepath: str"""
        logger.info("Caching stream.")
        with open(filepath, "wb") as f:
            for element in stream:
                marshal.dump(element, f)

    def get_stream(self):
        """Return the stream"""
        with open(self.tmp_filename) as f:
            try:
                while True:
                    yield marshal.load(f)
            except EOFError:
                pass


class FileStreamFactory(object):
    """Factory that produces a stream out of files of the same type.
    To generate a stream, the class :
    - locate all the corresponding files
    - sort them by part_id
    - generate a stream for each of them
    - chain the streams together
    """

    def __init__(self, dirpath, content, crawler_metakeys, part_id=None):
        """Initiate a factory

        :param dirpath: the path to the directory that contains crawl files
        :type dirpath: str
        :param content: basename of a content, eg. urlids
        :type content: str
        :param crawler_metakeys: the metadata generated by the crawler
        :type crawler_metakeys: dict
        :param part_id: select a partition, if None, stream all
            partitions in order
        :type part_id: int
        """
        self.dirpath = dirpath
        self.part_id = part_id
        self._crawler_metakeys = crawler_metakeys

        # pre-check on file basename
        found = False
        for f in Feature.get_features():
            for stream_def in f.get_streams_def():
                if content == stream_def.FILE:
                    self.stream_def = stream_def
                    found = True
        if found:
            self.content = content
        else:
            raise Exception("{} is not a known raw file basename".format(content))

    def _get_file_regexp(self):
        """Return a string representing a regex for the filenames
        that correspond to the desired content and part_id
        :returns: _sre.SRE_Pattern -- a compiled regex

        """
        template = '{}.txt.{}.gz'
        wildcard = '*'
        pattern = template.format(self.content,
                                  self.part_id if (self.part_id is not None) else wildcard)
        return re.compile(pattern)

    def _get_file_list(self, crawler_metakeys):
        """Return the list of files to stream
        :param crawler_metakeys: the crawler metakeys
                           (as returned by crawler_metakeys getter)
        :type crawler_metakeys: dict
        :returns: list -- the list of filepaths to stream

        """
        if self.content not in crawler_metakeys:
            logger.warning("No entry for %s found", self.content)
            return []

        def relocate(path):
            return os.path.join(self.dirpath, os.path.basename(path))
        file_list = [relocate(f) for f in crawler_metakeys[self.content]]
        file_list = [f for f in file_list if
                     self._get_file_regexp().match(os.path.basename(f))]
        return file_list

    def _get_stream_from_file(self, input_file):
        """Build the stream corresponding to a file
        :param input_file: the input file
        :type input_file: file
        :returns: generator -- the stream corresponding to the input file
                               with each field correctly casted.

        """
        return self.stream_def.load_iterator(split_file(input_file))

    def get_stream(self):
        """:returns: generator -- the desired generator"""
        regexp = self._get_file_regexp()
        logger.info('Streaming files with regexp {}'.format(regexp.pattern))

        files = self._get_file_list(self._crawler_metakeys)

        # sort files by part_id
        # assume file name format to be `basename.txt.part_id.gz`
        ordered_files = sorted(files, key=get_part_id_from_filename)

        streams = []
        for filename in ordered_files:
            f = gzip.open(filename)
            streams.append(self._get_stream_from_file(f))
        return itertools.chain(*streams)


class DataStreamFactory(object):
    """An abstract class for data stream factories
    Data stream factories create streams for given type of data :
    host, query_strings, etc.
    They take raw file streams and filter and process the stream
    to extract relevant data
    """
    def __init__(self, file_stream_factory, crawler_metakeys):
        """Constructor
        :param file_stream_factory: the factory that generate the raw file streams
        :type file_stream_factory: FileStreamFactory
        :param crawler_metakeys: the metadata generated by the crawler
        :type crawler_metakeys: dict
        """
        self._file_stream_factory = file_stream_factory
        self._crawler_metakeys = crawler_metakeys

    def set_file_stream_factory(self, stream_factory):
        """A setter for the stream factory.
        This function was implemented for test purpose
        :param stream_factory: the new stream factory
        :type stream_factory: FileStreamFactory
        """
        self._file_stream_factory = stream_factory

    def get_stream(self):
        """A method that generate a stream on the desired data"""
        raise NotImplementedError()


class ProtocolStreamFactory(DataStreamFactory):
    def __init__(self, file_stream_factory, crawler_metakeys):
        """Constructor
        :param file_stream_factory: the factory that generate the raw file streams
        :type file_stream_factory: FileStreamFactory
        :param crawler_metakeys: the metadata generated by the crawler
        :type crawler_metakeys: dict
        """
        super(self.__class__, self).__init__(file_stream_factory,
                                             crawler_metakeys)
        #we support only these protocols,
        #the other protocols are filtered out from the stream
        self._allowed_protocols = ["http", "https"]

    def get_stream(self):
        """Create a generator for the protocls
        The generator creates tuples (urlid, protocol)
        :returns: generator
        """
        base_stream = self._file_stream_factory.get_stream()
        max_crawled_urlid = get_max_crawled_urlid(self._crawler_metakeys)
        urlid_idx = IdStreamDef.field_idx("id")
        protocol_idx = IdStreamDef.field_idx("protocol")

        for url in base_stream:
            urlid = url[urlid_idx]
            protocol = url[protocol_idx]
            protocol = unicode(protocol, encoding="utf-8")
            if protocol not in self._allowed_protocols:
                logger.debug("Not supported protocol : '%s'", protocol)
                continue

            if urlid > max_crawled_urlid:
                raise StopIteration
            else:
                yield urlid, protocol


class HostStreamFactory(DataStreamFactory):
    def __init__(self, file_stream_factory, crawler_metakeys):
        """Constructor
        :param file_stream_factory: the factory that generate the raw file streams
        :type file_stream_factory: FileStreamFactory
        :param crawler_metakeys: the metadata generated by the crawler
        :type crawler_metakeys: dict
        """
        super(self.__class__, self).__init__(file_stream_factory,
                                             crawler_metakeys)

    def get_stream(self):
        """Create a generator for the hosts
        The generator creates tuples (urlid, host)
        :returns: generator
        """
        base_stream = self._file_stream_factory.get_stream()
        max_crawled_urlid = get_max_crawled_urlid(self._crawler_metakeys)
        urlid_idx = IdStreamDef.field_idx("id")
        host_idx = IdStreamDef.field_idx("host")
        for url in base_stream:
            urlid = url[urlid_idx]
            host = url[host_idx]
            host = unicode(host, encoding="utf-8")
            if urlid > max_crawled_urlid:
                raise StopIteration
            else:
                yield urlid, host


class PathStreamFactory(DataStreamFactory):
    def __init__(self, file_stream_factory, crawler_metakeys):
        """Constructor
        :param file_stream_factory: the factory that generate the raw file streams
        :type file_stream_factory: FileStreamFactory
        :param crawler_metakeys: the metadata generated by the crawler
        :type crawler_metakeys: dict
        """
        super(self.__class__, self).__init__(file_stream_factory,
                                             crawler_metakeys)

    def get_stream(self):
        """Create a generator for the paths
        The generator creates tuples (urlid, path)
        :returns: generator
        """
        base_stream = self._file_stream_factory.get_stream()
        max_crawled_urlid = get_max_crawled_urlid(self._crawler_metakeys)
        urlid_idx = IdStreamDef.field_idx("id")
        path_idx = IdStreamDef.field_idx("path")
        for url in base_stream:
            urlid = url[urlid_idx]
            path = url[path_idx]
            path = unicode(path, encoding="utf-8")
            parsed_path = urlsplit(path)
            path = parsed_path.path
            if urlid > max_crawled_urlid:
                raise StopIteration
            else:
                yield urlid, path


class QueryStringStreamFactory(DataStreamFactory):
    def __init__(self, file_stream_factory, crawler_metakeys):
        """Constructor
        :param file_stream_factory: the factory that generate the raw file streams
        :type file_stream_factory: FileStreamFactory
        :param crawler_metakeys: the metadata generated by the crawler
        :type crawler_metakeys: dict
        """
        super(self.__class__, self).__init__(file_stream_factory,
                                             crawler_metakeys)
        # a flag to indicate wheter we should return the raw query string
        # or parse it
        # for instance if the flag is True the stream will generate stuff like
        # - 0, {"foo": "bar", "baz": "qux"}
        # if the flag is False the stream will generate stuff like
        # - 0, "foo=bar&baz=qux"
        self._parse_string = True

    @property
    def parse_string(self):
        return self._parse_string

    @parse_string.setter
    def parse_string(self, value):
        self._parse_string = value

    def get_stream(self):
        """Create a generator for the query strings
        The generator returns the raw query string.
        For instance "category=shopping&page=2"
        :returns: generator
        """
        base_stream = self._file_stream_factory.get_stream()
        max_crawled_urlid = get_max_crawled_urlid(self._crawler_metakeys)
        urlid_idx = IdStreamDef.field_idx("id")
        query_string_index = IdStreamDef.field_idx("query_string")
        for url in base_stream:
            urlid = url[urlid_idx]
            if urlid > max_crawled_urlid:
                raise StopIteration

            if len(url) < query_string_index + 1:
                query_string = ''
            else:
                query_string = url[query_string_index]
                query_string = unicode(query_string, encoding="utf-8")
                query_string = query_string[1:]
            if self._parse_string:
                query_string = parse_qs(query_string)
            yield urlid, query_string


class MetadataStreamFactory(DataStreamFactory):
    def __init__(self, file_stream_factory, content_type, crawler_metakeys):
        """Constructor
        :param file_stream_factory: the factory that generate the raw file streams
        :type file_stream_factory: FileStreamFactory
        :param content_type: the kind of metadata: "title", "h1", etc.
                             that we want to figure in the generated streams
        :type content_type: str
        :param crawler_metakeys: the metadata generated by the crawler
        :type crawler_metakeys: dict
        """
        super(self.__class__, self).__init__(file_stream_factory,
                                             crawler_metakeys)
        #init class specific attributes
        self._content_type = content_type
        self._content_type_code = CONTENT_TYPE_NAME_TO_ID[self._content_type]

    @property
    def content_type(self):
        return self._content_type

    def get_stream(self):
        """Create a generator for the metadata
        The generator creates tuples (urlid, list_metadata)
        :returns: generator
        """
        base_stream = self._file_stream_factory.get_stream()
        max_crawled_urlid = get_max_crawled_urlid(self._crawler_metakeys)
        content_type_idx = ContentsStreamDef.field_idx("content_type")
        text_idx = ContentsStreamDef.field_idx("txt")
        for urlid, lines in itertools.groupby(base_stream,
                                              key=lambda url: url[0]):
            result = []
            for line in lines:
                metadata_code = line[content_type_idx]
                if metadata_code != self._content_type_code:
                    continue
                metadata = line[text_idx]
                metadata = unicode(metadata, encoding="utf-8")
                result.append(metadata)
            if urlid > max_crawled_urlid:
                raise StopIteration
            if len(result) == 0:
                #if we do not have corresponding metadata do not generate
                #an element for this urlid
                continue
            yield urlid, result


def get_nb_crawled_urls(data_directory_path):
    """Return the number of crawled urls
    :param data_directory_path: the path to the directory
                                that contains the crawl data
    :type data_directory_path: str
    :returns: int

    """
    crawler_metakeys = load_crawler_metakeys(data_directory_path)
    urlinfos_stream_factory = FileStreamFactory(data_directory_path,
                                                "urlinfos",
                                                crawler_metakeys)
    max_crawled_urlid = get_max_crawled_urlid(crawler_metakeys)
    return _get_nb_crawled_urls_from_stream(
        urlinfos_stream_factory.get_stream(),
        max_crawled_urlid
    )


def _get_nb_crawled_urls_from_stream(urlinfos_stream, max_crawled_urlid):
    """Helper function (mainly here to make tests easier
    Return the number of available pages
    :param urlinfos_stream: a stream from the urlinfos files
    :type urlinfos_stream: generator
    :param max_crawled_urlid : the highest urlid corresponding
                               to a crawled page
    :type max_crawled_urlid: int
    :returns: int

    """
    result = 0
    urlid_idx = InfosStreamDef.field_idx("id")
    httpcode_idx = InfosStreamDef.field_idx("http_code")
    for urlinfo in urlinfos_stream:
        urlid = urlinfo[urlid_idx]
        httpcode = urlinfo[httpcode_idx]
        if urlid > max_crawled_urlid:
            break  # there will be no more crawled url
        if httpcode == 0:
            continue
        result += 1
    return result
