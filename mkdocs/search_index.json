{
    "docs": [
        {
            "location": "/", 
            "text": "Simpleflow\n\n\n\n[![Pypi Status](https://badge.fury.io/py/simpleflow.png)](http://badge.fury.io/py/simpleflow) [![Build Status](https://travis-ci.org/botify-labs/simpleflow.svg?branch=master)](https://travis-ci.org/botify-labs/simpleflow)\n\n\n\n\nSimpleflow is a Python library that provides abstractions to write programs in\nthe \ndistributed dataflow paradigm\n.\nIt coordinates the execution of distributed tasks with \nAmazon SWF\n.\n\n\nIt relies on \nfutures\n to describe the dependencies between tasks. A \nFuture\n object\nmodels the asynchronous execution of a computation that may end.  It tries to mimic\nthe interface of the Python \nconcurrent.futures\n library.\n\n\nFeatures\n\n\n\n\nProvides a \nFuture\n abstraction to define dependencies between tasks.\n\n\nDefine asynchronous tasks from callables.\n\n\nHandle workflows with Amazon SWF.\n\n\nImplement replay behavior like the Amazon Flow framework.\n\n\nHandle retry of tasks that failed.\n\n\nAutomatically register decorated tasks.\n\n\nEncodes/decodes large fields to S3 objects transparently (aka \"jumbo fields\").\n\n\nHandle the completion of a decision with more than 100 tasks.\n\n\nProvides a local executor to check a workflow without Amazon SWF (see\n  \nsimpleflow --local\n command).\n\n\nProvides decider and activity worker process for execution with Amazon SWF.\n\n\nShips with the \nsimpleflow\n command. \nsimpleflow --help\n for more information\n  about the commands it supports.\n\n\n\n\nYou can read more in the \nFeatures\n section of the documentation.\n\n\nOverview\n\n\nPlease read and even run the \ndemo\n script to have a quick glance of\n\nsimpleflow\n commands. To run the \ndemo\n  you will need to start decider\nand activity worker processes.\n\n\nStart a decider with::\n\n\n$ simpleflow decider.start --domain TestDomain --task-list test examples.basic.BasicWorkflow\n\n\n\n\n\nStart an activity worker with::\n\n\n$ simpleflow worker.start --domain TestDomain --task-list quickstart\n\n\n\n\n\nThen execute \n./extras/demo\n.\n\n\nMore informations\n\n\nRead the main documentation at \nhttps://botify-labs.github.io/simpleflow/\n.", 
            "title": "Intro"
        }, 
        {
            "location": "/#simpleflow", 
            "text": "[![Pypi Status](https://badge.fury.io/py/simpleflow.png)](http://badge.fury.io/py/simpleflow) [![Build Status](https://travis-ci.org/botify-labs/simpleflow.svg?branch=master)](https://travis-ci.org/botify-labs/simpleflow)  Simpleflow is a Python library that provides abstractions to write programs in\nthe  distributed dataflow paradigm .\nIt coordinates the execution of distributed tasks with  Amazon SWF .  It relies on  futures  to describe the dependencies between tasks. A  Future  object\nmodels the asynchronous execution of a computation that may end.  It tries to mimic\nthe interface of the Python  concurrent.futures  library.", 
            "title": "Simpleflow"
        }, 
        {
            "location": "/#features", 
            "text": "Provides a  Future  abstraction to define dependencies between tasks.  Define asynchronous tasks from callables.  Handle workflows with Amazon SWF.  Implement replay behavior like the Amazon Flow framework.  Handle retry of tasks that failed.  Automatically register decorated tasks.  Encodes/decodes large fields to S3 objects transparently (aka \"jumbo fields\").  Handle the completion of a decision with more than 100 tasks.  Provides a local executor to check a workflow without Amazon SWF (see\n   simpleflow --local  command).  Provides decider and activity worker process for execution with Amazon SWF.  Ships with the  simpleflow  command.  simpleflow --help  for more information\n  about the commands it supports.   You can read more in the  Features  section of the documentation.", 
            "title": "Features"
        }, 
        {
            "location": "/#overview", 
            "text": "Please read and even run the  demo  script to have a quick glance of simpleflow  commands. To run the  demo   you will need to start decider\nand activity worker processes.  Start a decider with::  $ simpleflow decider.start --domain TestDomain --task-list test examples.basic.BasicWorkflow  Start an activity worker with::  $ simpleflow worker.start --domain TestDomain --task-list quickstart  Then execute  ./extras/demo .", 
            "title": "Overview"
        }, 
        {
            "location": "/#more-informations", 
            "text": "Read the main documentation at  https://botify-labs.github.io/simpleflow/ .", 
            "title": "More informations"
        }, 
        {
            "location": "/installation/", 
            "text": "Installation\n\n\nFrom the PyPI (recommended)\n\n\n$ pip install -U simpleflow\n\n\n\n\n\nFrom Source\n\n\nsimpleflow is actively developed on \nGithub\n.\n\n\nYou can clone the public repo:\n\n\n$ git clone https://github.com/botify-labs/simpleflow\n\n\n\n\n\nOr download one of the following:\n\n\n\n\ntarball\n\n\nzipball\n\n\n\n\nOnce you have the source, you can install it into your site-packages with ::\n\n\n$ python setup.py install", 
            "title": "Installation"
        }, 
        {
            "location": "/installation/#installation", 
            "text": "", 
            "title": "Installation"
        }, 
        {
            "location": "/installation/#from-the-pypi-recommended", 
            "text": "$ pip install -U simpleflow", 
            "title": "From the PyPI (recommended)"
        }, 
        {
            "location": "/installation/#from-source", 
            "text": "simpleflow is actively developed on  Github .  You can clone the public repo:  $ git clone https://github.com/botify-labs/simpleflow  Or download one of the following:   tarball  zipball   Once you have the source, you can install it into your site-packages with ::  $ python setup.py install", 
            "title": "From Source"
        }, 
        {
            "location": "/architecture/standalone/", 
            "text": "Standalone mode\n\n\n\n\nWarning\n\n\nThis mode is well-suited for development but not for production, since\nit uses the local machine alone to take decisions and process activity\ntasks.\n\n\n\n\nIn this mode, your local machine will act as:\n\n\n\n\nworkflow starter\n: it submits the workflow to Amazon SWF (via the \nStartWorkflowExecution\n\n  endpoint)\n\n\ndecider\n: it polls SWF for decisions to take (via \nPollForDecisionTask\n) then takes\n  decisions with the workflow class you passed and submit decisions to SWF\n  (via \nRespondDecisionTaskCompleted\n)\n\n\nactivity worker\n: it polls SWF for activity tasks to execute (via \nPollForActivityTask\n)\n  then processes the task and submit the result to SWF, either via \nRespondActivityTaskCompleted\n\n  if successful, or via \nRespondActivityTaskFailed\n if not.\n\n\n\n\nUsually you'll want to tune the number of decider and worker processes that\nwill be spawned by the command. The command will look like:\n\nsimpleflow standalone --domain TestDomain \n\\\n\n    --nb-deciders \n1\n --nb-workers \n1\n \n\\\n\n    examples.basic.BasicWorkflow --input \n[1]\n\n\n\n\nYour process tree will look like:\n\n/code/simpleflow# ps auxf|grep simpleflow\nPID     COMMAND\n  6 ... simpleflow standalone examples.basic.BasicWorkflow --input [1]\n 10 ...  \\_ simpleflow Decider(payload=DeciderPoller.start, nb_children=1)[running]\n 12 ...  |   \\_ simpleflow DeciderPoller(task_list=basic-1234)[processing]\n 14 ...  |       \\_ simpleflow DeciderPoller(task_list=basic-1234)[deciding]\n 11 ...  \\_ simpleflow Worker(payload=ActivityPoller.start, nb_children=1)[running]\n 13 ...      \\_ simpleflow ActivityPoller(task_list=basic-1234)[polling]\n\n\n\nUnder your command, you now see 2 subprocesses:\n\n\n\n\na \nDecider supervisor\n process, that forks to one or more \nDeciderPoller\n processes\n\n\na \nWorker supervisor\n process, that forks to one or more \nActivityPoller\n processes\n\n\n\n\nEach poller then forks when doing real work related to SWF. Here we're in the middle of\na decision for the workflow, and no activity task is running.", 
            "title": "Standalone"
        }, 
        {
            "location": "/architecture/standalone/#standalone-mode", 
            "text": "Warning  This mode is well-suited for development but not for production, since\nit uses the local machine alone to take decisions and process activity\ntasks.   In this mode, your local machine will act as:   workflow starter : it submits the workflow to Amazon SWF (via the  StartWorkflowExecution \n  endpoint)  decider : it polls SWF for decisions to take (via  PollForDecisionTask ) then takes\n  decisions with the workflow class you passed and submit decisions to SWF\n  (via  RespondDecisionTaskCompleted )  activity worker : it polls SWF for activity tasks to execute (via  PollForActivityTask )\n  then processes the task and submit the result to SWF, either via  RespondActivityTaskCompleted \n  if successful, or via  RespondActivityTaskFailed  if not.   Usually you'll want to tune the number of decider and worker processes that\nwill be spawned by the command. The command will look like: simpleflow standalone --domain TestDomain  \\ \n    --nb-deciders  1  --nb-workers  1   \\ \n    examples.basic.BasicWorkflow --input  [1]   Your process tree will look like: /code/simpleflow# ps auxf|grep simpleflow\nPID     COMMAND\n  6 ... simpleflow standalone examples.basic.BasicWorkflow --input [1]\n 10 ...  \\_ simpleflow Decider(payload=DeciderPoller.start, nb_children=1)[running]\n 12 ...  |   \\_ simpleflow DeciderPoller(task_list=basic-1234)[processing]\n 14 ...  |       \\_ simpleflow DeciderPoller(task_list=basic-1234)[deciding]\n 11 ...  \\_ simpleflow Worker(payload=ActivityPoller.start, nb_children=1)[running]\n 13 ...      \\_ simpleflow ActivityPoller(task_list=basic-1234)[polling]  Under your command, you now see 2 subprocesses:   a  Decider supervisor  process, that forks to one or more  DeciderPoller  processes  a  Worker supervisor  process, that forks to one or more  ActivityPoller  processes   Each poller then forks when doing real work related to SWF. Here we're in the middle of\na decision for the workflow, and no activity task is running.", 
            "title": "Standalone mode"
        }, 
        {
            "location": "/architecture/multiprocess/", 
            "text": "Multiprocess architecture\n\n\n\n\nNote\n\n\nThis architecture is currently the recommended way to deploy simpleflow in\nproduction. This may change in the future in favor of the Kubernetes\narchitecture described in the \nfollowing section\n.\n\n\n\n\nIf you're not familiar with the 3 standard roles around an SWF setup, go read the\n\nprevious section about Standalone architecture\n. In this setup, the\n3 roles are potentially distributed on different machines:\n\n\n\n  \n\n\n\n\n\nA few notes about this schema:\n\n\n\n\nthe \nworkflow started\n role is generally behind a web app or an automated system depending on your\n  use case ; it's pretty uncommon to have workflows launched manually via the \nsimpleflow\n command-line.\n\n\nthe \nactivity workers\n can be distributed on many nodes, possibly with \nautoscaling\n mechanisms.\n\n\nthe \ndeciders\n on the other hand are usually only installed on a few machines, and don't need\n  autoscaling.", 
            "title": "Multiprocess"
        }, 
        {
            "location": "/architecture/multiprocess/#multiprocess-architecture", 
            "text": "Note  This architecture is currently the recommended way to deploy simpleflow in\nproduction. This may change in the future in favor of the Kubernetes\narchitecture described in the  following section .   If you're not familiar with the 3 standard roles around an SWF setup, go read the previous section about Standalone architecture . In this setup, the\n3 roles are potentially distributed on different machines:  \n     A few notes about this schema:   the  workflow started  role is generally behind a web app or an automated system depending on your\n  use case ; it's pretty uncommon to have workflows launched manually via the  simpleflow  command-line.  the  activity workers  can be distributed on many nodes, possibly with  autoscaling  mechanisms.  the  deciders  on the other hand are usually only installed on a few machines, and don't need\n  autoscaling.", 
            "title": "Multiprocess architecture"
        }, 
        {
            "location": "/architecture/kubernetes/", 
            "text": "Kubernetes architecture\n\n\n\n\nWarning\n\n\nThis architecture is currently in alpha mode and should only be used\nat your own risks.\n\n\n\n\nThis architecture splits activity workers in two parts:\n\n\n\n\nthe \npoller\n that only polls activity tasks from SWF and boot a corresponding\n  Kubernetes job for each activity task.\n\n\nthe \nKubernetes job\n processing an activity task, that doesn't poll but handle\n  the communication with SWF afterward: hearbeats, response when activity is finished.\n\n\n\n\n\n  \n\n\n\n\n\nThere are a few limitations with this design:\n\n\n\n\nthe initial implementation done in \n#313\n\n  assumes that pollers are run in a Kubernetes cluster \nor\n that the decider has a local\n  working \nkubectl\n configuration.\n\n\nby design, this architecture doesn't guarantee that tasks will have a \nstart timestamp\n\n  coherent with the moment the task really started to execute. You will get the timestamp\n  of the moment the activity was polled from SWF. This may be addressed in the future.\n\n\nrelated to last point: if your cluster doesn't have sufficient resources to schedule\n  the jobs, and it waits for too long, you may get a heartbeat (or start to close or\n  schedule to close) timeout triggering. So be careful and have your cluster scale as\n  needed.", 
            "title": "Kubernetes"
        }, 
        {
            "location": "/architecture/kubernetes/#kubernetes-architecture", 
            "text": "Warning  This architecture is currently in alpha mode and should only be used\nat your own risks.   This architecture splits activity workers in two parts:   the  poller  that only polls activity tasks from SWF and boot a corresponding\n  Kubernetes job for each activity task.  the  Kubernetes job  processing an activity task, that doesn't poll but handle\n  the communication with SWF afterward: hearbeats, response when activity is finished.   \n     There are a few limitations with this design:   the initial implementation done in  #313 \n  assumes that pollers are run in a Kubernetes cluster  or  that the decider has a local\n  working  kubectl  configuration.  by design, this architecture doesn't guarantee that tasks will have a  start timestamp \n  coherent with the moment the task really started to execute. You will get the timestamp\n  of the moment the activity was polled from SWF. This may be addressed in the future.  related to last point: if your cluster doesn't have sufficient resources to schedule\n  the jobs, and it waits for too long, you may get a heartbeat (or start to close or\n  schedule to close) timeout triggering. So be careful and have your cluster scale as\n  needed.", 
            "title": "Kubernetes architecture"
        }, 
        {
            "location": "/quickstart/", 
            "text": "Quickstart\n\n\nLet's take a simple example that computes the result of \n(x + 1) * 2\n. You\nwill find this example in \nexamples/basic.py\n.\n\n\nWe need to declare the functions as activities to make them available:\n\n\nfrom\n \nsimpleflow\n \nimport\n \n(\n\n    \nactivity\n,\n\n    \nWorkflow\n,\n\n    \nfutures\n,\n\n\n)\n\n\n\n@activity.with_attributes\n(\ntask_list\n=\nquickstart\n,\n \nversion\n=\nexample\n)\n\n\ndef\n \nincrement\n(\nx\n):\n\n    \nreturn\n \nx\n \n+\n \n1\n\n\n\n@activity.with_attributes\n(\ntask_list\n=\nquickstart\n,\n \nversion\n=\nexample\n)\n\n\ndef\n \ndouble\n(\nx\n):\n\n    \nreturn\n \nx\n \n*\n \n2\n\n\n\n@activity.with_attributes\n(\ntask_list\n=\nquickstart\n,\n \nversion\n=\nexample\n)\n\n\ndef\n \ndelay\n(\nt\n,\n \nx\n):\n\n    \ntime\n.\nsleep\n(\nt\n)\n\n    \nreturn\n \nx\n\n\n\n\n\nAnd then define the workflow itself in a \nexample.py\n file:\n\n\nclass\n \nBasicWorkflow\n(\nWorkflow\n):\n\n    \nname\n \n=\n \nbasic\n\n    \nversion\n \n=\n \nexample\n\n    \ntask_list\n \n=\n \nexample\n\n\n    \ndef\n \nrun\n(\nself\n,\n \nx\n,\n \nt\n=\n30\n):\n\n        \ny\n \n=\n \nself\n.\nsubmit\n(\nincrement\n,\n \nx\n)\n\n        \nyy\n \n=\n \nself\n.\nsubmit\n(\ndelay\n,\n \nt\n,\n \ny\n)\n\n        \nz\n \n=\n \nself\n.\nsubmit\n(\ndouble\n,\n \ny\n)\n\n\n        \nprint\n(\n({x} + 1) * 2 = {result}\n.\nformat\n(\n\n            \nx\n=\nx\n,\n\n            \nresult\n=\nz\n.\nresult\n))\n\n        \nfutures\n.\nwait\n(\nyy\n,\n \nz\n)\n\n        \nreturn\n \nz\n.\nresult\n\n\n\n\n\nNow check that the workflow works locally with an integer \"x\" and a wait value \"t\"::\n\n\n$ simpleflow workflow.start --local examples.basic.BasicWorkflow --input \n[1, 5]\n\n(1 + 1) * 2 = 4\n\n\n\n\n\ninput\n is encoded in JSON format and can contain the list of \npositional\n\narguments such as \n'[1, 1]\n or a \ndict\n with the \nargs\n and \nkwargs\n keys\nsuch as \n{\nargs\n: [1], \nkwargs\n: {}}\n, \n{\nkwargs\n: {\nx\n: 1}}\n, or\n\n'{\nargs\n: [1], \nkwargs\n: {\nt\n: 5}}'\n.\n\n\nNow that you are confident that the workflow should work, you can run it on\nAmazon SWF with the \nstandalone\n command::\n\n\n$ simpleflow standalone --domain TestDomain examples.basic.BasicWorkflow --input \n[1, 5]\n\n\n\n\n\n\nThe \nstandalone\n command sets an unique task list and manage all the processes\nthat are needed to execute the workflow: decider, activity worker, and a client\nthat starts the workflow. It is very convenient for testing a workflow by\nexecuting it with SWF during the development steps or integration tests.\n\n\nLet's take a closer look to the workflow definition.\n\n\nIt is a \nclass\n that inherits from \nsimpleflow.Workflow\n:\n\n\nclass\n \nBasicWorkflow\n(\nWorkflow\n):\n\n\n\n\n\nIt defines 3 class attributes:\n\n\n\n\nname\n, the name of the SWF workflow type.\n\n\nversion\n, the version of the SWF workflow type. It is currently provided\n  only for labeling a workflow.\n\n\ntask_list\n, the default task list (see it as a dynamically created queue)\n  where decision tasks for this workflow will be sent. Any \ndecider\n that\n  listens on this task list can handle this workflow. This value can be\n  overrided by the simpleflow commands and objects.\n\n\n\n\nIt also implements the \nrun\n method that takes two arguments: \nx\n and\n\nt=30\n (i.e. \nt\n is optional and has the default value \n30\n). These\narguments are passed with the \n--input\n option. The \nrun\n method\ndescribes the workflow and how its tasks should execute.\n\n\nEach time a decider takes a decision task, it executes again the \nrun\n\nfrom the start. When the workflow execution starts, it evaluates \ny =\nself.submit(increment, x)\n for the first time. \ny\n holds a future in state\n\nPENDING\n. The execution continues with the line \nyy = self.submit(delay, t,\ny)\n. \nyy\n holds another future in state \nPENDING\n. This state means the task\nhas not been scheduled. Now execution still continue in the \nrun\n method\nwith the line \nz = self.submit(double, y)\n. Here it needs the value of the\n\ny\n future to evaluate the \ndouble\n activity. As the execution cannot\ncontinues, the decider schedules the task \nincrement\n. \nyy\n is not a\ndependency for any task so it is not scheduled.\n\n\nOnce the decider has scheduled the task for \ny\n, it sleeps and waits for an\nevent to be waken up. This happens when the \nincrement\n task completes.\nSWF schedules a decision task. A decider takes it and executes the\n\nBasicWorkflow.run\n method again from the start. It evalues the line \ny\n= self.submit(increment, x)\n. The task associated with the \ny\n future has\ncompleted. Hence \ny\n is in state \nFINISHED\n and contains the value \n2\n in\n\ny.result\n. The execution continues until it blocks. It goes by \nyy =\nself.submit(delay, t, y)\n that stays the same. Then it reaches \nz =\nself.submit(double, y)\n. It gets the value of \ny.result\n and \nz\n now holds a\nfuture in state \nPENDING\n. Execution reaches the line with the \nprint\n. It\nblocks here because \nz.result\n is not available. The decider schedules the\ntask backs by the \nz\n future: \ndouble(y)\n. The workflow execution continues\nso forth by evaluating the \nBasicWorkflow.run\n again from the start until\nit finishes.", 
            "title": "Quickstart"
        }, 
        {
            "location": "/quickstart/#quickstart", 
            "text": "Let's take a simple example that computes the result of  (x + 1) * 2 . You\nwill find this example in  examples/basic.py .  We need to declare the functions as activities to make them available:  from   simpleflow   import   ( \n     activity , \n     Workflow , \n     futures ,  )  @activity.with_attributes ( task_list = quickstart ,   version = example )  def   increment ( x ): \n     return   x   +   1  @activity.with_attributes ( task_list = quickstart ,   version = example )  def   double ( x ): \n     return   x   *   2  @activity.with_attributes ( task_list = quickstart ,   version = example )  def   delay ( t ,   x ): \n     time . sleep ( t ) \n     return   x   And then define the workflow itself in a  example.py  file:  class   BasicWorkflow ( Workflow ): \n     name   =   basic \n     version   =   example \n     task_list   =   example \n\n     def   run ( self ,   x ,   t = 30 ): \n         y   =   self . submit ( increment ,   x ) \n         yy   =   self . submit ( delay ,   t ,   y ) \n         z   =   self . submit ( double ,   y ) \n\n         print ( ({x} + 1) * 2 = {result} . format ( \n             x = x , \n             result = z . result )) \n         futures . wait ( yy ,   z ) \n         return   z . result   Now check that the workflow works locally with an integer \"x\" and a wait value \"t\"::  $ simpleflow workflow.start --local examples.basic.BasicWorkflow --input  [1, 5] \n(1 + 1) * 2 = 4  input  is encoded in JSON format and can contain the list of  positional \narguments such as  '[1, 1]  or a  dict  with the  args  and  kwargs  keys\nsuch as  { args : [1],  kwargs : {}} ,  { kwargs : { x : 1}} , or '{ args : [1],  kwargs : { t : 5}}' .  Now that you are confident that the workflow should work, you can run it on\nAmazon SWF with the  standalone  command::  $ simpleflow standalone --domain TestDomain examples.basic.BasicWorkflow --input  [1, 5]   The  standalone  command sets an unique task list and manage all the processes\nthat are needed to execute the workflow: decider, activity worker, and a client\nthat starts the workflow. It is very convenient for testing a workflow by\nexecuting it with SWF during the development steps or integration tests.  Let's take a closer look to the workflow definition.  It is a  class  that inherits from  simpleflow.Workflow :  class   BasicWorkflow ( Workflow ):   It defines 3 class attributes:   name , the name of the SWF workflow type.  version , the version of the SWF workflow type. It is currently provided\n  only for labeling a workflow.  task_list , the default task list (see it as a dynamically created queue)\n  where decision tasks for this workflow will be sent. Any  decider  that\n  listens on this task list can handle this workflow. This value can be\n  overrided by the simpleflow commands and objects.   It also implements the  run  method that takes two arguments:  x  and t=30  (i.e.  t  is optional and has the default value  30 ). These\narguments are passed with the  --input  option. The  run  method\ndescribes the workflow and how its tasks should execute.  Each time a decider takes a decision task, it executes again the  run \nfrom the start. When the workflow execution starts, it evaluates  y =\nself.submit(increment, x)  for the first time.  y  holds a future in state PENDING . The execution continues with the line  yy = self.submit(delay, t,\ny) .  yy  holds another future in state  PENDING . This state means the task\nhas not been scheduled. Now execution still continue in the  run  method\nwith the line  z = self.submit(double, y) . Here it needs the value of the y  future to evaluate the  double  activity. As the execution cannot\ncontinues, the decider schedules the task  increment .  yy  is not a\ndependency for any task so it is not scheduled.  Once the decider has scheduled the task for  y , it sleeps and waits for an\nevent to be waken up. This happens when the  increment  task completes.\nSWF schedules a decision task. A decider takes it and executes the BasicWorkflow.run  method again from the start. It evalues the line  y\n= self.submit(increment, x) . The task associated with the  y  future has\ncompleted. Hence  y  is in state  FINISHED  and contains the value  2  in y.result . The execution continues until it blocks. It goes by  yy =\nself.submit(delay, t, y)  that stays the same. Then it reaches  z =\nself.submit(double, y) . It gets the value of  y.result  and  z  now holds a\nfuture in state  PENDING . Execution reaches the line with the  print . It\nblocks here because  z.result  is not available. The decider schedules the\ntask backs by the  z  future:  double(y) . The workflow execution continues\nso forth by evaluating the  BasicWorkflow.run  again from the start until\nit finishes.", 
            "title": "Quickstart"
        }, 
        {
            "location": "/features/swf_layer/", 
            "text": "SWF Object Layer\n\n\nsimpleflow\n includes a \nswf\n module that is an object-oriented wrapper for the\n\nboto.swf\n library, used to access the \nAmazon Simple Workflow\n service.\n\n\nIt aims to provide:\n\n\n\n\nModelisation\n: Swf entities and concepts are to be manipulated through \nModels\n and\n  \nQuerySets\n (any ressemblance with the Django API would not be a coincidence).\n\n\nHigh-level Events, History\n: A higher level of abstractions over SWF \nevents\n and\n  \nhistory\n. Events are implemented as stateful objects aware of their own state and\n  possible transitions. History enhances the events flow description, and can be\n  compiled to check its integrity and the activities statuses transitions.\n\n\nDecisions\n: Stateful abstractions above the SWF decision making system.\n\n\nActors\n: SWF actors base implementation such as a \nDecider\n or an activity\n  \nWorker\n from which the user can easily inherit to implement its own\n  decision/processing model.\n\n\n\n\nSettings\n\n\n\n\nBug\n\n\nThe informations in this \"Settings\" section may be outdated, they need some love.\n\n\n\n\nMandatory:\n\n\n\n\naws_access_key_id\n\n\naws_secret_access_key\n\n\n\n\nOptional:\n\n\n\n\nregion\n\n\n\n\nSettings are found respectively in:\n\n\nA credential file a \n.swf\n file in the user's home directory:\n\n\n[credentials]\n\n\naws_access_key_id\n=\naws_access_key_id\n\n\naws_secret_access_key\n=\naws_secret_access_key\n\n\n\n[defaults]\n\n\nregion\n=\nus-east-1\n\n\n\n\n\nThe following environment variables\n    - \nAWS_ACCESS_KEY_ID\n\n    - \nAWS_SECRET_ACCESS_KEY\n\n    - \nregion\n\n\nIf neither of the previous methods were used, you can still set the AWS credentials with \nswf.settings.set\n:\n\n\n \nimport\n \nswf.settings\n\n\n \nswf\n.\nsettings\n.\nset\n(\naws_access_key_id\n=\nMYAWSACCESSKEYID\n,\n\n\n...\n                  \naws_secret_access_key\n=\nMYAWSSECRETACCESSKEY\n,\n\n\n...\n                  \nregion\n=\nREGION\n)\n\n\n# And then you\nre good to go...\n\n\n \nqueryset\n \n=\n \nDomainQuery\n()\n\n\n \nqueryset\n.\nall\n()\n\n\n[\nDomain\n(\ntest1\n),\n \nDomain\n(\ntest2\n)]\n\n\n\n\n\nExample usage\n\n\nModels\n\n\nSimple Workflow entities such as domains, workflow types, workflow executions and activity types are to be\nmanipulated through swf using \nmodels\n. They are immutable \nswf\n objects representations providing an\ninterface to objects attributes, local/remote objects synchronization and changes watch between these\nlocal and remote objects.\n\n\n# Models reside in the swf.models module\n\n\n \nfrom\n \nswf.models\n \nimport\n \nDomain\n,\n \nWorkflowType\n,\n \nWorkflowExecution\n,\n \nActivityType\n\n\n\n# Once imported you\nre ready to create a local model instance\n\n\n \nD\n \n=\n \nDomain\n(\n\n    \nmy-test-domain-name\n,\n\n    \ndescription\n=\nmy-test-domain-description\n,\n\n    \nretention_period\n=\n60\n\n\n)\n\n\n\n# a Domain model local instance has been created, but nothing has been\n\n\n# sent to amazon. To do so, you have to save it.\n\n\n \nD\n.\nsave\n()\n\n\n\n\n\nNow you have a local \nDomain\n model object, and if no errors were raised, the \nsave\n method have saved\namazon-side. Sometimes you won't be able to know if the model you're manipulating has an upstream version:\nwhether you've acquired it through a queryset, or the remote object has been deleted for example.\nFortunately, models are shipped with a set of functions to make sure your local objects keep synced and\nconsistent.\n\n\n# Exists method lets you know if your model instance has an upstream version\n\n\n \nD\n.\nexists\n\n\nTrue\n\n\n\n# What if changes have been made to the remote object?\n\n\n# synced  and changes methods help ensuring local and remote models\n\n\n# are still synced and which changes have been made (in the case below\n\n\n# nothing has changed)\n\n\n \nD\n.\nis_synced\n\n\nTrue\n\n\n \nD\n.\nchanges\n\n\nModelDiff\n()\n\n\n\n\n\nWhat if your local object is out of sync? Models \nupstream\n method will fetch the remote version of\nyour object and will build a new model instance using its attributes.\n\n\n \nD\n.\nis_synced\n\n\nFalse\n\n\n \nD\n.\nchanges\n\n\nModelDiff\n(\n\n    \nDifference\n(\nstatus\n,\n \nREGISTERED\n,\n \nDEPRECATED\n)\n\n\n)\n\n\n\n# Let\ns pull the upstream version\n\n\n \nD\n \n=\n \nD\n.\nupstream\n()\n\n\n \nD\n.\nis_synced\n\n\nTrue\n\n\n \nD\n.\nchanges\n\n\nModelDiff\n()\n\n\n\n\n\nQuerySets\n\n\nModels can be retrieved and instantiated via querysets. To continue over the django comparison,\nthey're behaving like django managers.\n\n\n# As querying for models needs a valid connection to amazon service,\n\n\n# Queryset objects cannot act as classmethods proxy and have to be instantiated;\n\n\n# most of the time against a Domain model instance\n\n\n \nfrom\n \nswf.querysets\n \nimport\n \nDomainQuerySet\n,\n \nWorkflowTypeQuerySet\n\n\n\n# Domain querysets can be instantiated directly\n\n\n \ndomain_qs\n \n=\n \nDomainQuerySet\n()\n\n\n \nworkflow_domain\n \n=\n \ndomain_qs\n.\nget\n(\nMyTestDomain\n)\n  \n# and specific model retieved via .get method\n\n\n \nworkflow_qs\n \n=\n \nWorkflowTypeQuerySet\n(\nworkflow_domain\n)\n  \n# queryset built against model instance example\n\n\n\n \nworkflow_qs\n.\nall\n()\n\n\n[\nWorkflowType\n(\nTestType1\n),\n \nWorkflowType\n(\nTestType2\n),]\n\n\n\n \nworkflow_qs\n.\nfilter\n(\nstatus\n=\nDEPRECATED\n)\n\n\n[\nWorkflowType\n(\nDeprecatedType1\n),]\n\n\n\n\n\nEvents\n\n\n(coming soon)\n\n\nHistory\n\n\n(coming soon)\n\n\nDecisions\n\n\n(coming soon)\n\n\nActors\n\n\nSWF workflows are based on a worker-decider pattern. Every actions in the flow is executed by a worker\nwhich runs supplied activity tasks. And every actions is the result of a decision taken by the decider\nreading the workflow events history and deciding what to do next. In order to ease the development of\nsuch workers and decider, \nswf\n exposes base classes for them located in the \nswf.actors\n submodule.\n\n\n\n\nAn \nActor\n must basically implement a \nstart\n and \nstop\n method and can actually inherits from whatever\n  runtime implementation you need: thread, gevent, multiprocess...\n\n\n\n\nclass\n \nActor\n(\nConnectedSWFObject\n):\n\n    \ndef\n \n__init__\n(\nself\n,\n \ndomain\n,\n \ntask_list\n)\n\n    \ndef\n \nstart\n(\nself\n):\n\n    \ndef\n \nstop\n(\nself\n):\n\n\n\n\n\n\n\nDecider\n base class implements the core functionality of a swf decider: polling for decisions tasks,\n  and sending back a decision task copleted decision. Every other special needs implementations are left\n  up to the user.\n\n\n\n\nclass\n \nDecider\n(\nActor\n):\n\n    \ndef\n \n__init__\n(\nself\n,\n \ndomain\n,\n \ntask_list\n)\n\n    \ndef\n \ncomplete\n(\nself\n,\n \ntask_token\n,\n \ndecisions\n=\nNone\n,\n \nexecution_context\n=\nNone\n)\n\n    \ndef\n \npoll\n(\nself\n,\n \ntask_list\n=\nNone\n,\n \nidentity\n=\nNone\n,\n \nmaximum_page_size\n=\nNone\n)\n\n\n\n\n\n\n\nWorker\n base class implements the core functionality of a swf worker whoes role is to process activity\n  tasks. It is basically able to poll for new activity tasks to process, send back a heartbeat to SWF\n  service in order to let it know it hasn't failed or crashed, and to complete, fail or cancel the activity\n  task it's processing.\n\n\n\n\nclass\n \nActivityWorker\n(\nActor\n):\n\n    \ndef\n \n__init__\n(\nself\n,\n \ndomain\n,\n \ntask_list\n)\n\n    \ndef\n \ncancel\n(\nself\n,\n \ntask_token\n,\n \ndetails\n=\nNone\n)\n\n    \ndef\n \ncomplete\n(\nself\n,\n \ntask_token\n,\n \nresult\n=\nNone\n)\n\n    \ndef\n \nfail\n(\nself\n,\n \ntask_token\n,\n \ndetails\n=\nNone\n,\n \nreason\n=\nNone\n)\n\n    \ndef\n \nheartbeat\n(\nself\n,\n \ntask_token\n,\n \ndetails\n=\nNone\n)\n\n    \ndef\n \npoll\n(\nself\n,\n \ntask_list\n=\nNone\n,\n \n**\nkwargs\n)", 
            "title": "SWF Object Layer"
        }, 
        {
            "location": "/features/swf_layer/#swf-object-layer", 
            "text": "simpleflow  includes a  swf  module that is an object-oriented wrapper for the boto.swf  library, used to access the  Amazon Simple Workflow  service.  It aims to provide:   Modelisation : Swf entities and concepts are to be manipulated through  Models  and\n   QuerySets  (any ressemblance with the Django API would not be a coincidence).  High-level Events, History : A higher level of abstractions over SWF  events  and\n   history . Events are implemented as stateful objects aware of their own state and\n  possible transitions. History enhances the events flow description, and can be\n  compiled to check its integrity and the activities statuses transitions.  Decisions : Stateful abstractions above the SWF decision making system.  Actors : SWF actors base implementation such as a  Decider  or an activity\n   Worker  from which the user can easily inherit to implement its own\n  decision/processing model.", 
            "title": "SWF Object Layer"
        }, 
        {
            "location": "/features/swf_layer/#settings", 
            "text": "Bug  The informations in this \"Settings\" section may be outdated, they need some love.   Mandatory:   aws_access_key_id  aws_secret_access_key   Optional:   region   Settings are found respectively in:  A credential file a  .swf  file in the user's home directory:  [credentials]  aws_access_key_id = aws_access_key_id  aws_secret_access_key = aws_secret_access_key  [defaults]  region = us-east-1   The following environment variables\n    -  AWS_ACCESS_KEY_ID \n    -  AWS_SECRET_ACCESS_KEY \n    -  region  If neither of the previous methods were used, you can still set the AWS credentials with  swf.settings.set :    import   swf.settings    swf . settings . set ( aws_access_key_id = MYAWSACCESSKEYID ,  ...                    aws_secret_access_key = MYAWSSECRETACCESSKEY ,  ...                    region = REGION )  # And then you re good to go...    queryset   =   DomainQuery ()    queryset . all ()  [ Domain ( test1 ),   Domain ( test2 )]", 
            "title": "Settings"
        }, 
        {
            "location": "/features/swf_layer/#example-usage", 
            "text": "", 
            "title": "Example usage"
        }, 
        {
            "location": "/features/swf_layer/#models", 
            "text": "Simple Workflow entities such as domains, workflow types, workflow executions and activity types are to be\nmanipulated through swf using  models . They are immutable  swf  objects representations providing an\ninterface to objects attributes, local/remote objects synchronization and changes watch between these\nlocal and remote objects.  # Models reside in the swf.models module    from   swf.models   import   Domain ,   WorkflowType ,   WorkflowExecution ,   ActivityType  # Once imported you re ready to create a local model instance    D   =   Domain ( \n     my-test-domain-name , \n     description = my-test-domain-description , \n     retention_period = 60  )  # a Domain model local instance has been created, but nothing has been  # sent to amazon. To do so, you have to save it.    D . save ()   Now you have a local  Domain  model object, and if no errors were raised, the  save  method have saved\namazon-side. Sometimes you won't be able to know if the model you're manipulating has an upstream version:\nwhether you've acquired it through a queryset, or the remote object has been deleted for example.\nFortunately, models are shipped with a set of functions to make sure your local objects keep synced and\nconsistent.  # Exists method lets you know if your model instance has an upstream version    D . exists  True  # What if changes have been made to the remote object?  # synced  and changes methods help ensuring local and remote models  # are still synced and which changes have been made (in the case below  # nothing has changed)    D . is_synced  True    D . changes  ModelDiff ()   What if your local object is out of sync? Models  upstream  method will fetch the remote version of\nyour object and will build a new model instance using its attributes.    D . is_synced  False    D . changes  ModelDiff ( \n     Difference ( status ,   REGISTERED ,   DEPRECATED )  )  # Let s pull the upstream version    D   =   D . upstream ()    D . is_synced  True    D . changes  ModelDiff ()", 
            "title": "Models"
        }, 
        {
            "location": "/features/swf_layer/#querysets", 
            "text": "Models can be retrieved and instantiated via querysets. To continue over the django comparison,\nthey're behaving like django managers.  # As querying for models needs a valid connection to amazon service,  # Queryset objects cannot act as classmethods proxy and have to be instantiated;  # most of the time against a Domain model instance    from   swf.querysets   import   DomainQuerySet ,   WorkflowTypeQuerySet  # Domain querysets can be instantiated directly    domain_qs   =   DomainQuerySet ()    workflow_domain   =   domain_qs . get ( MyTestDomain )    # and specific model retieved via .get method    workflow_qs   =   WorkflowTypeQuerySet ( workflow_domain )    # queryset built against model instance example    workflow_qs . all ()  [ WorkflowType ( TestType1 ),   WorkflowType ( TestType2 ),]    workflow_qs . filter ( status = DEPRECATED )  [ WorkflowType ( DeprecatedType1 ),]", 
            "title": "QuerySets"
        }, 
        {
            "location": "/features/swf_layer/#events", 
            "text": "(coming soon)", 
            "title": "Events"
        }, 
        {
            "location": "/features/swf_layer/#history", 
            "text": "(coming soon)", 
            "title": "History"
        }, 
        {
            "location": "/features/swf_layer/#decisions", 
            "text": "(coming soon)", 
            "title": "Decisions"
        }, 
        {
            "location": "/features/swf_layer/#actors", 
            "text": "SWF workflows are based on a worker-decider pattern. Every actions in the flow is executed by a worker\nwhich runs supplied activity tasks. And every actions is the result of a decision taken by the decider\nreading the workflow events history and deciding what to do next. In order to ease the development of\nsuch workers and decider,  swf  exposes base classes for them located in the  swf.actors  submodule.   An  Actor  must basically implement a  start  and  stop  method and can actually inherits from whatever\n  runtime implementation you need: thread, gevent, multiprocess...   class   Actor ( ConnectedSWFObject ): \n     def   __init__ ( self ,   domain ,   task_list ) \n     def   start ( self ): \n     def   stop ( self ):    Decider  base class implements the core functionality of a swf decider: polling for decisions tasks,\n  and sending back a decision task copleted decision. Every other special needs implementations are left\n  up to the user.   class   Decider ( Actor ): \n     def   __init__ ( self ,   domain ,   task_list ) \n     def   complete ( self ,   task_token ,   decisions = None ,   execution_context = None ) \n     def   poll ( self ,   task_list = None ,   identity = None ,   maximum_page_size = None )    Worker  base class implements the core functionality of a swf worker whoes role is to process activity\n  tasks. It is basically able to poll for new activity tasks to process, send back a heartbeat to SWF\n  service in order to let it know it hasn't failed or crashed, and to complete, fail or cancel the activity\n  task it's processing.   class   ActivityWorker ( Actor ): \n     def   __init__ ( self ,   domain ,   task_list ) \n     def   cancel ( self ,   task_token ,   details = None ) \n     def   complete ( self ,   task_token ,   result = None ) \n     def   fail ( self ,   task_token ,   details = None ,   reason = None ) \n     def   heartbeat ( self ,   task_token ,   details = None ) \n     def   poll ( self ,   task_list = None ,   ** kwargs )", 
            "title": "Actors"
        }, 
        {
            "location": "/features/command_line/", 
            "text": "Command Line\n\n\nSimpleflow comes with a \nsimpleflow\n command-line utility that can be used to list workflows against SWF, boot decider or activity workers (with multiprocessing), and a few other goodies.\n\n\nList Workflow Executions\n\n\n$ simpleflow workflow.list TestDomain\nbasic-example-1438722273  basic  OPEN\n\n\n\n\n\nWorkflow Execution Status\n\n\n$ simpleflow --header workflow.info TestDomain basic-example-1438722273\ndomain      workflow_type.name    workflow_type.version      task_list  workflow_id               run_id                                          tag_list      execution_time  input\nTestDomain  basic                 example                               basic-example-1438722273  22QFVi362TnCh6BdoFgkQFlocunh24zEOemo1L12Yl5Go=                          1.70  {u\nargs\n: [1], u\nkwargs\n: {}}\n\n\n\n\n\nTasks Status\n\n\nYou can check the status of the workflow execution with::\n\n\n$ simpleflow --header workflow.tasks DOMAIN WORKFLOW_ID [RUN_ID] --nb-tasks 3\n$ simpleflow --header workflow.tasks TestDomain basic-example-1438722273\nTasks                     Last State    Last State Time             Scheduled Time\nexamples.basic.increment  scheduled     2015-08-04 23:04:34.510000  2015-08-04 23:04:34.510000\n$ simpleflow --header workflow.tasks TestDomain basic-example-1438722273\nTasks                     Last State    Last State Time             Scheduled Time\nexamples.basic.double     completed     2015-08-04 23:06:19.200000  2015-08-04 23:06:17.738000\nexamples.basic.delay      completed     2015-08-04 23:08:18.402000  2015-08-04 23:06:17.738000\nexamples.basic.increment  completed     2015-08-04 23:06:17.503000  2015-08-04 23:04:34.510000\n\n\n\n\n\nProfiling\n\n\nYou can profile the execution of the workflow with::\n\n\n$ simpleflow --header workflow.profile TestDomain basic-example-1438722273\nTask                                 Last State    Scheduled           Time Scheduled  Start               Time Running  End                 Percentage of total time\nactivity-examples.basic.double-1     completed     2015-08-04 23:06              0.07  2015-08-04 23:06            1.39  2015-08-04 23:06                        1.15\nactivity-examples.basic.increment-1  completed     2015-08-04 23:04            102.20  2015-08-04 23:06            0.79  2015-08-04 23:06                        0.65\n\n\n\n\n\nControlling SWF access\n\n\nThe SWF region is controlled by the environment variable \nAWS_DEFAULT_REGION\n. This variable\ncomes from the legacy \"simple-workflow\" project. The option might be exposed through a\n\n--region\n option in the future (if you want that, please open an issue).\n\n\nThe SWF domain is controlled by the \n--domain\n on most simpleflow commands. It can also\nbe set via the \nSWF_DOMAIN\n environment variable. In case both are supplied, the\ncommand-line value takes precedence over the environment variable.\n\n\nNote that some simpleflow commands expect the domain to be passed as a positionnal argument.\nIn that case the environment variable has no effect for now.\n\n\nThe number of retries for accessing SWF can be controlled via \nSWF_CONNECTION_RETRIES\n\n(defaults to 5).\n\n\nThe identity of SWF activity workers and deciders can be controlled via \nSIMPLEFLOW_IDENTITY\n\nwhich should be a JSON-serialized string representing \n{ \nkey\n: \nvalue\n }\n pairs that\nadds up (or override) the basic identity provided by simpleflow. If some value is null in\nthis JSON map, then the key is removed from the final SWF identity.\n\n\nControlling log verbosity\n\n\nYou can control log verbosity via the \nLOG_LEVEL\n environment variable. Default is \nINFO\n. For instance,\nthe following command will start a decider with \nDEBUG\n logs:\n\n\n$ LOG_LEVEL=DEBUG simpleflow decider.start --domain TestDomain --task-list test examples.basic.BasicWorkflow", 
            "title": "Command Line"
        }, 
        {
            "location": "/features/command_line/#command-line", 
            "text": "Simpleflow comes with a  simpleflow  command-line utility that can be used to list workflows against SWF, boot decider or activity workers (with multiprocessing), and a few other goodies.", 
            "title": "Command Line"
        }, 
        {
            "location": "/features/command_line/#list-workflow-executions", 
            "text": "$ simpleflow workflow.list TestDomain\nbasic-example-1438722273  basic  OPEN", 
            "title": "List Workflow Executions"
        }, 
        {
            "location": "/features/command_line/#workflow-execution-status", 
            "text": "$ simpleflow --header workflow.info TestDomain basic-example-1438722273\ndomain      workflow_type.name    workflow_type.version      task_list  workflow_id               run_id                                          tag_list      execution_time  input\nTestDomain  basic                 example                               basic-example-1438722273  22QFVi362TnCh6BdoFgkQFlocunh24zEOemo1L12Yl5Go=                          1.70  {u args : [1], u kwargs : {}}", 
            "title": "Workflow Execution Status"
        }, 
        {
            "location": "/features/command_line/#tasks-status", 
            "text": "You can check the status of the workflow execution with::  $ simpleflow --header workflow.tasks DOMAIN WORKFLOW_ID [RUN_ID] --nb-tasks 3\n$ simpleflow --header workflow.tasks TestDomain basic-example-1438722273\nTasks                     Last State    Last State Time             Scheduled Time\nexamples.basic.increment  scheduled     2015-08-04 23:04:34.510000  2015-08-04 23:04:34.510000\n$ simpleflow --header workflow.tasks TestDomain basic-example-1438722273\nTasks                     Last State    Last State Time             Scheduled Time\nexamples.basic.double     completed     2015-08-04 23:06:19.200000  2015-08-04 23:06:17.738000\nexamples.basic.delay      completed     2015-08-04 23:08:18.402000  2015-08-04 23:06:17.738000\nexamples.basic.increment  completed     2015-08-04 23:06:17.503000  2015-08-04 23:04:34.510000", 
            "title": "Tasks Status"
        }, 
        {
            "location": "/features/command_line/#profiling", 
            "text": "You can profile the execution of the workflow with::  $ simpleflow --header workflow.profile TestDomain basic-example-1438722273\nTask                                 Last State    Scheduled           Time Scheduled  Start               Time Running  End                 Percentage of total time\nactivity-examples.basic.double-1     completed     2015-08-04 23:06              0.07  2015-08-04 23:06            1.39  2015-08-04 23:06                        1.15\nactivity-examples.basic.increment-1  completed     2015-08-04 23:04            102.20  2015-08-04 23:06            0.79  2015-08-04 23:06                        0.65", 
            "title": "Profiling"
        }, 
        {
            "location": "/features/command_line/#controlling-swf-access", 
            "text": "The SWF region is controlled by the environment variable  AWS_DEFAULT_REGION . This variable\ncomes from the legacy \"simple-workflow\" project. The option might be exposed through a --region  option in the future (if you want that, please open an issue).  The SWF domain is controlled by the  --domain  on most simpleflow commands. It can also\nbe set via the  SWF_DOMAIN  environment variable. In case both are supplied, the\ncommand-line value takes precedence over the environment variable.  Note that some simpleflow commands expect the domain to be passed as a positionnal argument.\nIn that case the environment variable has no effect for now.  The number of retries for accessing SWF can be controlled via  SWF_CONNECTION_RETRIES \n(defaults to 5).  The identity of SWF activity workers and deciders can be controlled via  SIMPLEFLOW_IDENTITY \nwhich should be a JSON-serialized string representing  {  key :  value  }  pairs that\nadds up (or override) the basic identity provided by simpleflow. If some value is null in\nthis JSON map, then the key is removed from the final SWF identity.", 
            "title": "Controlling SWF access"
        }, 
        {
            "location": "/features/command_line/#controlling-log-verbosity", 
            "text": "You can control log verbosity via the  LOG_LEVEL  environment variable. Default is  INFO . For instance,\nthe following command will start a decider with  DEBUG  logs:  $ LOG_LEVEL=DEBUG simpleflow decider.start --domain TestDomain --task-list test examples.basic.BasicWorkflow", 
            "title": "Controlling log verbosity"
        }, 
        {
            "location": "/features/program_tasks/", 
            "text": "Execution of Tasks as Programs\n\n\nThe \nsimpleflow.execute\n module allows to define functions that will be\nexecuted as a program.\n\n\nThere are two modes:\n\n\n\n\nConvert the definition of a fonction as a command line.\n\n\nExecute a Python function in another process.\n\n\n\n\nPlease refer to the \nsimpleflow.tests.test_activity\n test module for\nfurther examples.\n\n\nExecuting a function as a command line\n\n\nLet's take the example of \nls\n:\n\n\n@execute.program\n()\n\n\ndef\n \nls\n():\n\n    \npass\n\n\n\n\n\nCalling \nls()\n in Python will execute the \nls\n command. Here the purpose of\nthe function definition is only to describe the command line. The reason for\nthis is to map a call in a workflow definition to a program to execute on the\ncommand line. The program may be written in any language whereas the workflow\ndefinition is in Python.\n\n\nExecuting a Python function in another process\n\n\nThe rationale for this feature is to execute a function with another\ninterpreter (such as pypy) or in another environment (virtualenv).\n\n\n@execute.python\n(\ninterpreter\n=\npypy\n)\n\n\ndef\n \ninc\n(\nxs\n):\n\n    \nreturn\n \n[\nx\n \n+\n \n1\n \nfor\n \nx\n \nin\n \nxs\n]\n\n\n\n\n\nCalling \ninc(range(10))\n in Python will execute the function with the\n\npypy\n interpreter found in the \n$PATH\n.\n\n\nLimitations\n\n\nThe main limitation comes from the need to serialize the arguments and the\nreturn values to pass them as strings. Hence all arguments and return values\nmust be convertible into JSON values.", 
            "title": "Program Tasks"
        }, 
        {
            "location": "/features/program_tasks/#execution-of-tasks-as-programs", 
            "text": "The  simpleflow.execute  module allows to define functions that will be\nexecuted as a program.  There are two modes:   Convert the definition of a fonction as a command line.  Execute a Python function in another process.   Please refer to the  simpleflow.tests.test_activity  test module for\nfurther examples.", 
            "title": "Execution of Tasks as Programs"
        }, 
        {
            "location": "/features/program_tasks/#executing-a-function-as-a-command-line", 
            "text": "Let's take the example of  ls :  @execute.program ()  def   ls (): \n     pass   Calling  ls()  in Python will execute the  ls  command. Here the purpose of\nthe function definition is only to describe the command line. The reason for\nthis is to map a call in a workflow definition to a program to execute on the\ncommand line. The program may be written in any language whereas the workflow\ndefinition is in Python.", 
            "title": "Executing a function as a command line"
        }, 
        {
            "location": "/features/program_tasks/#executing-a-python-function-in-another-process", 
            "text": "The rationale for this feature is to execute a function with another\ninterpreter (such as pypy) or in another environment (virtualenv).  @execute.python ( interpreter = pypy )  def   inc ( xs ): \n     return   [ x   +   1   for   x   in   xs ]   Calling  inc(range(10))  in Python will execute the function with the pypy  interpreter found in the  $PATH .", 
            "title": "Executing a Python function in another process"
        }, 
        {
            "location": "/features/program_tasks/#limitations", 
            "text": "The main limitation comes from the need to serialize the arguments and the\nreturn values to pass them as strings. Hence all arguments and return values\nmust be convertible into JSON values.", 
            "title": "Limitations"
        }, 
        {
            "location": "/features/jumbo_fields/", 
            "text": "Jumbo Fields\n\n\n\n\nWarning\n\n\nThis feature is in \nbeta\n mode and subject to changes. Any feedback is appreciated.\n\n\n\n\nFor some use cases, you want to be able to have fields larger than what SWF accepts\n(which is maximum 32K bytes on the largest ones, \ninput\n and \nresult\n, and lower for\nsome others, as documented \nhere\n).\n\n\nSimpleflow allows to transparently translate such fields to objects stored on AWS\nS3. The format is then the following:\n\n\nsimpleflow+s3://jumbo-bucket/with/optional/prefix/5d7191af-[...]-cdd39a31ba61 5242880\n\n\n\n\n\nFormat\n\n\nThe format provides a pseudo-S3 address as a first word. The \nsimpleflow+s3://\n\nprefix is here for implementation purposes, and may be extended later with other\nbackends such as \nsimpleflow+ssh\n or \nsimpleflow+gs\n.\n\n\nThe second word provides the length of the object in bytes, so a client parsing\nthe SWF history can decide if it's worth it to pull/decode the object.\n\n\nFor now jumbo fields are limited to 5MB in size. Simpleflow will perform disk caching\nfor this feature to avoid issuing too many queries to S3, which would slow down\nthe deciders especially. Disk cache is located at \n/tmp/simpleflow-cache\n and is\nlimited to 1GB, with a LRU eviction strategy. It's performed with the\n\nDiskCache library\n.\n\n\nConfiguration\n\n\nYou have to configure an environment variable to tell simpleflow where to store\nthings (which implicitly enables the feature by the way):\n\n\nSIMPLEFLOW_JUMBO_FIELDS_BUCKET=jumbo-bucket/with/optional/prefix\n\n\n\n\n\nAnd ensure your deciders and activity workers have access to this S3 bucket (\ns3:GetObject\n and\n\ns3:PutObject\n should be enough, but please test it first).\n\n\n\n\nWarning on bucket name length\n\n\nThe overhead of the signature format is maximum 91 chars at this point (fixed protocol\nand UUID width, and max 5M = 5242880 for the size part). So you should ensure\nthat your bucket + directory is not longer than 256 - 91 = 165 chars, else\nyou may not be able to get a working jumbo field signature for tiny fields.\nIn that case stripping the signature would only break things down the road\nin unpredictable and hard to debug ways, so simpleflow will raise.", 
            "title": "Jumbo Fields"
        }, 
        {
            "location": "/features/jumbo_fields/#jumbo-fields", 
            "text": "Warning  This feature is in  beta  mode and subject to changes. Any feedback is appreciated.   For some use cases, you want to be able to have fields larger than what SWF accepts\n(which is maximum 32K bytes on the largest ones,  input  and  result , and lower for\nsome others, as documented  here ).  Simpleflow allows to transparently translate such fields to objects stored on AWS\nS3. The format is then the following:  simpleflow+s3://jumbo-bucket/with/optional/prefix/5d7191af-[...]-cdd39a31ba61 5242880", 
            "title": "Jumbo Fields"
        }, 
        {
            "location": "/features/jumbo_fields/#format", 
            "text": "The format provides a pseudo-S3 address as a first word. The  simpleflow+s3:// \nprefix is here for implementation purposes, and may be extended later with other\nbackends such as  simpleflow+ssh  or  simpleflow+gs .  The second word provides the length of the object in bytes, so a client parsing\nthe SWF history can decide if it's worth it to pull/decode the object.  For now jumbo fields are limited to 5MB in size. Simpleflow will perform disk caching\nfor this feature to avoid issuing too many queries to S3, which would slow down\nthe deciders especially. Disk cache is located at  /tmp/simpleflow-cache  and is\nlimited to 1GB, with a LRU eviction strategy. It's performed with the DiskCache library .", 
            "title": "Format"
        }, 
        {
            "location": "/features/jumbo_fields/#configuration", 
            "text": "You have to configure an environment variable to tell simpleflow where to store\nthings (which implicitly enables the feature by the way):  SIMPLEFLOW_JUMBO_FIELDS_BUCKET=jumbo-bucket/with/optional/prefix  And ensure your deciders and activity workers have access to this S3 bucket ( s3:GetObject  and s3:PutObject  should be enough, but please test it first).   Warning on bucket name length  The overhead of the signature format is maximum 91 chars at this point (fixed protocol\nand UUID width, and max 5M = 5242880 for the size part). So you should ensure\nthat your bucket + directory is not longer than 256 - 91 = 165 chars, else\nyou may not be able to get a working jumbo field signature for tiny fields.\nIn that case stripping the signature would only break things down the road\nin unpredictable and hard to debug ways, so simpleflow will raise.", 
            "title": "Configuration"
        }, 
        {
            "location": "/features/signals/", 
            "text": "Signals\n\n\n\n\nWarning\n\n\nThis feature is in \nbeta\n mode and subject to changes. Any feedback is appreciated.\n\n\n\n\nSignals are handled via two methods: \nWorkflow.signal\n and \nWorkflow.wait_signal\n.\nThey are currently only implemented with SWF.\n\n\nSignaling a workflow\n\n\nThe \nWorkflow.signal\n method sends a signal to one or several workflows.\n\n\ndef\n \nrun\n(\nself\n):\n\n    \n# Send to self, parent and children\n\n    \nfuture\n \n=\n \nself\n.\nsubmit\n(\nself\n.\nsignal\n(\nsignal_name\n,\n \n*\nargs\n,\n \n**\nkwargs\n))\n\n\n    \n# Send to specific workflow\n\n    \nfuture\n \n=\n \nself\n.\nsubmit\n(\nself\n.\nsignal\n(\nsignal_name\n,\n \nworkflow_id\n,\n \nrun_id\n,\n \n*\nargs\n,\n \n**\nkwargs\n))\n\n\n\n\n\nThe future will be finished, its result being *args and **kwargs, as soon as at least one workflow has been signaled\n(including oneself).\n\n\nWaiting for a signal\n\n\nThe \nWorkflow.wait_signal\n returns a \nFuture\n which result is the signal input.\n\n\ndef\n \nrun\n(\nself\n):\n\n    \nfuture\n \n=\n \nself\n.\nsubmit\n(\nself\n.\nwait_signal\n(\nsignal_name\n))\n\n    \nresult\n \n=\n \nfuture\n.\nresult\n\n\n\n\n\nNaturally, one isn't forced to wait on the future result:\n\n\ndef\n \nrun\n(\nself\n):\n\n    \nmy_signal\n \n=\n \nself\n.\nsubmit\n(\nself\n.\nwait_signal\n(\nsignal_name\n))\n\n    \nif\n \nmy_signal\n.\nfinished\n:\n\n        \n# Something happened\n\n        \nself\n.\nprocess\n(\nmy_signal\n.\nresult\n)\n\n\n\n\n\nLimitations\n\n\n\n\nsignals cannot be reset; they can be overwritten though (only the latest one count)\n\n\nderive from futures.Future to add the timestamp or counter and better names? This would bypass the \"reset\" issue too\n\n\n\n\nOne way to handle recurrent signals is by using \nevent_id\n's (available with \nWorkflow.get_event_details\n). For\ninstance, when receiving a signal, check that a marker with the same name does not exist or is in the past (lower\nevent ID); if so, the signal is new, so process it and create a marker.\n\n\nImplementation\n\n\nThe \nswf.executor.signal\n method returns a \nswf.SignalTask\n instance. Its \nschedule\n method\nreturns an \nExternalWorkflowExecutionDecision\n containing the given signal, sent either to the running workflow or\nthe specified one.\n\n\nThis decision results in a \nSignalExternalWorkflowExecutionInitiated\n followed (if all's well) by a\n\nSignalExternalWorkflowExecutionInitiated\n in the sender's history; from these events, we create first a running,\nthen a completed future. (It can also fail, for instance if the workflow doesn't exist.)\n\n\nThe receiver gets a \nWorkflowExecutionSignaled\n with the signal name, input and external (i.e. sender) information.\nWe may want every known workflow to be signaled too:  if \npropagate=True\n is passed to \nWorkflow.signal\n, the\nsignal is propagated to the parent and children of the workflow.\n\n\nSince we propagate using \nSignalWorkflowExecution\n, not a decision, the target doesn't have the\n\nexternalWorkflowExecution\n information; so we pass \n__workflow_id\n and \n__run_id\n in the input.", 
            "title": "Signals"
        }, 
        {
            "location": "/features/signals/#signals", 
            "text": "Warning  This feature is in  beta  mode and subject to changes. Any feedback is appreciated.   Signals are handled via two methods:  Workflow.signal  and  Workflow.wait_signal .\nThey are currently only implemented with SWF.", 
            "title": "Signals"
        }, 
        {
            "location": "/features/signals/#signaling-a-workflow", 
            "text": "The  Workflow.signal  method sends a signal to one or several workflows.  def   run ( self ): \n     # Send to self, parent and children \n     future   =   self . submit ( self . signal ( signal_name ,   * args ,   ** kwargs )) \n\n     # Send to specific workflow \n     future   =   self . submit ( self . signal ( signal_name ,   workflow_id ,   run_id ,   * args ,   ** kwargs ))   The future will be finished, its result being *args and **kwargs, as soon as at least one workflow has been signaled\n(including oneself).", 
            "title": "Signaling a workflow"
        }, 
        {
            "location": "/features/signals/#waiting-for-a-signal", 
            "text": "The  Workflow.wait_signal  returns a  Future  which result is the signal input.  def   run ( self ): \n     future   =   self . submit ( self . wait_signal ( signal_name )) \n     result   =   future . result   Naturally, one isn't forced to wait on the future result:  def   run ( self ): \n     my_signal   =   self . submit ( self . wait_signal ( signal_name )) \n     if   my_signal . finished : \n         # Something happened \n         self . process ( my_signal . result )", 
            "title": "Waiting for a signal"
        }, 
        {
            "location": "/features/signals/#limitations", 
            "text": "signals cannot be reset; they can be overwritten though (only the latest one count)  derive from futures.Future to add the timestamp or counter and better names? This would bypass the \"reset\" issue too   One way to handle recurrent signals is by using  event_id 's (available with  Workflow.get_event_details ). For\ninstance, when receiving a signal, check that a marker with the same name does not exist or is in the past (lower\nevent ID); if so, the signal is new, so process it and create a marker.", 
            "title": "Limitations"
        }, 
        {
            "location": "/features/signals/#implementation", 
            "text": "The  swf.executor.signal  method returns a  swf.SignalTask  instance. Its  schedule  method\nreturns an  ExternalWorkflowExecutionDecision  containing the given signal, sent either to the running workflow or\nthe specified one.  This decision results in a  SignalExternalWorkflowExecutionInitiated  followed (if all's well) by a SignalExternalWorkflowExecutionInitiated  in the sender's history; from these events, we create first a running,\nthen a completed future. (It can also fail, for instance if the workflow doesn't exist.)  The receiver gets a  WorkflowExecutionSignaled  with the signal name, input and external (i.e. sender) information.\nWe may want every known workflow to be signaled too:  if  propagate=True  is passed to  Workflow.signal , the\nsignal is propagated to the parent and children of the workflow.  Since we propagate using  SignalWorkflowExecution , not a decision, the target doesn't have the externalWorkflowExecution  information; so we pass  __workflow_id  and  __run_id  in the input.", 
            "title": "Implementation"
        }, 
        {
            "location": "/development/", 
            "text": "Development\n\n\nRequirements\n\n\n\n\nCPython 2.7 or 3.6 (recommended)\n\n\nPypy 2.5+\n\n\n\n\nNB about Pypy: all tests pass but some parts of the deciders may not work ; Pypy\nsupport is mostly for activity workers where you need the performance boost.\n\n\nDevelopment environment\n\n\nA \nDockerfile\n is provided to help development on non-Linux machines.\n\n\nYou can build a \nsimpleflow\n image with:\n\n\n./script/docker-build\n\n\n\n\n\nAnd use it with:\n\n\n./script/docker-run\n\n\n\n\n\nIt will then mount your current directory inside the container and pass the\nmost relevant variables (your \nAWS_*\n credentials for instance).\n\n\nRunning tests\n\n\nYou can run tests with:\n\n\n./script/test\n\n\n\n\n\nAny parameter passed to this script is propagated to the underlying call to \npy.test\n.\nThis wrapper script sets some environment variables which control the behavior of\nsimpleflow during tests:\n\n\n\n\nSIMPLEFLOW_CLEANUP_PROCESSES\n: set to \nyes\n in tests, so tests will clean up child\n  processes after each test case. You can set it to an empty string (\n) or omit it if\n  outside \nscript/test\n if you want to debug things and take care of it yourself.\n\n\nSIMPLEFLOW_ENV\n: set to \ntest\n in tests, which changes some constants to ease or\n  speed up tests.\n\n\nSWF_CONNECTION_RETRIES\n: set to \n1\n in tests, which avoids having too many retries\n  on the SWF API calls (5 by default in production).\n\n\nSIMPLEFLOW_VCR_RECORD_MODE\n: set to \nnone\n in tests, which avoids running requests\n  against the real SWF endpoints in tests. If you need to update cassettes, see\n  \ntests/integration/README.md\n\n\n\n\nReproducing Travis failures\n\n\nIt might happen that a test fails on \nTravis\n\nand you want to reproduce locally. Travis has a \nhelpful section in their docs\n\nabout reproducing such issues. As of 2017, simpleflow builds run on 12.04 containers on\nthe Travis infrastructure. So you can get close to the Travis setup with something like:\n\n\ndocker run -it \\\n  -u travis \\\n  -e DEBIAN_FRONTEND=noninteractive \\\n  -e PYTHONDONTWRITEBYTECODE=true \\\n  -v $(pwd):/botify-labs/simpleflow \\\n  quay.io/travisci/travis-python /bin/bash\n\n\n\n\n\nThen you may want to follow your failed build commands to reproduce the errors.\n\n\nFor instance on pypy builds the commands look like:\n\n\nsudo apt-get install ca-certificates libssl1.0.0\ncd botify-labs/simpleflow\nsource ~/virtualenv/pypy/bin/activate\npip install .\npip install -r requirements-dev.txt\nrm -rf build/\n./script/test -vv\n\n\n\n\n\nRelease\n\n\nIn order to release a new version, you'll need credentials on pypi.python.org for this\nsoftware, as long as write access to this repository. Ask via an issue if needed.\n\n\nThe release process is then automated behind a script:\n\n\n./script/release", 
            "title": "Development"
        }, 
        {
            "location": "/development/#development", 
            "text": "", 
            "title": "Development"
        }, 
        {
            "location": "/development/#requirements", 
            "text": "CPython 2.7 or 3.6 (recommended)  Pypy 2.5+   NB about Pypy: all tests pass but some parts of the deciders may not work ; Pypy\nsupport is mostly for activity workers where you need the performance boost.", 
            "title": "Requirements"
        }, 
        {
            "location": "/development/#development-environment", 
            "text": "A  Dockerfile  is provided to help development on non-Linux machines.  You can build a  simpleflow  image with:  ./script/docker-build  And use it with:  ./script/docker-run  It will then mount your current directory inside the container and pass the\nmost relevant variables (your  AWS_*  credentials for instance).", 
            "title": "Development environment"
        }, 
        {
            "location": "/development/#running-tests", 
            "text": "You can run tests with:  ./script/test  Any parameter passed to this script is propagated to the underlying call to  py.test .\nThis wrapper script sets some environment variables which control the behavior of\nsimpleflow during tests:   SIMPLEFLOW_CLEANUP_PROCESSES : set to  yes  in tests, so tests will clean up child\n  processes after each test case. You can set it to an empty string ( ) or omit it if\n  outside  script/test  if you want to debug things and take care of it yourself.  SIMPLEFLOW_ENV : set to  test  in tests, which changes some constants to ease or\n  speed up tests.  SWF_CONNECTION_RETRIES : set to  1  in tests, which avoids having too many retries\n  on the SWF API calls (5 by default in production).  SIMPLEFLOW_VCR_RECORD_MODE : set to  none  in tests, which avoids running requests\n  against the real SWF endpoints in tests. If you need to update cassettes, see\n   tests/integration/README.md", 
            "title": "Running tests"
        }, 
        {
            "location": "/development/#reproducing-travis-failures", 
            "text": "It might happen that a test fails on  Travis \nand you want to reproduce locally. Travis has a  helpful section in their docs \nabout reproducing such issues. As of 2017, simpleflow builds run on 12.04 containers on\nthe Travis infrastructure. So you can get close to the Travis setup with something like:  docker run -it \\\n  -u travis \\\n  -e DEBIAN_FRONTEND=noninteractive \\\n  -e PYTHONDONTWRITEBYTECODE=true \\\n  -v $(pwd):/botify-labs/simpleflow \\\n  quay.io/travisci/travis-python /bin/bash  Then you may want to follow your failed build commands to reproduce the errors.  For instance on pypy builds the commands look like:  sudo apt-get install ca-certificates libssl1.0.0\ncd botify-labs/simpleflow\nsource ~/virtualenv/pypy/bin/activate\npip install .\npip install -r requirements-dev.txt\nrm -rf build/\n./script/test -vv", 
            "title": "Reproducing Travis failures"
        }, 
        {
            "location": "/development/#release", 
            "text": "In order to release a new version, you'll need credentials on pypi.python.org for this\nsoftware, as long as write access to this repository. Ask via an issue if needed.  The release process is then automated behind a script:  ./script/release", 
            "title": "Release"
        }, 
        {
            "location": "/contributing/", 
            "text": "Contributing guidelines\n\n\nIn General\n\n\n\n\nPEP 8\n, when sensible.\n\n\nTest ruthlessly. Write docs for new features.\n\n\nEven more important than Test-Driven Development, \nHuman-Driven Development\n.\n\n\n\n\nIn Particular\n\n\n\n\nWarning\n\n\nTHE WHOLE SECTION IS OUT OF DATE.\n\n\n\n\nQuestions, Feature Requests, Bug Reports, and Feedback\n\n\n... should all be reported on the \nGithub Issue Tracker\n.\n\n\nSetting Up for Local Development\n\n\n\n\nFork \nsimpleflow\n_ on Github.\n\n\n\n\nClone your fork::\n\n\n$ git clone \n/botify-labs/simpleflow.git\n\n\n\n\n\n\nMake your virtualenv and install dependencies. If you have virtualenv and virtualenvwrapper_, run::\n\n\n$ mkvirtualenv simpleflow\n$ cd simpleflow\n$ pip install -r requirements-dev.txt\n\n\n\n\n\n\nIf you don't have virtualenv and virtualenvwrapper, you can install both using \nvirtualenv-burrito\n.\n\n\n\n\n\n\nGit Branch Structure\n\n\nsimpleflow used to have a separated \ndevel\n branch but is now using only one,\nmain branch \nmaster\n, that contains what will be released in the next version.\nThis branch is (hopefully) always stable.\n\n\nPull Requests\n\n\n\n\n\n\nCreate a new local branch. ::\n\n\n$ git checkout -b name-of-feature\n\n\n\n\n\n\nCommit your changes. Write \ngood commit messages\n.\n\n\n$ git commit -m \"Detailed commit message\"\n$ git push origin name-of-feature\n\n\n\n\n\n\nBefore submitting a pull request, check the following:\n\n\n\n\n\n\nIf the pull request adds functionality, it should be tested and the docs should be updated.\n\n\n\n\n\n\nThe pull request should work on Python 2.7 and PyPy. Use \ntox\n to verify that it does.\n\n\n\n\n\n\nSubmit a pull request to the \nmaster\n branch.\n\n\n\n\n\n\nRunning tests\n\n\nTo run all the tests in your current virtual environment: ::\n\n\n$ ./script/test\n\n\n\n\n\nThis is what Travis CI does on each environment.\n\n\nIf you want to simulate what Travis CI does, you can approach that by running a container\nfrom them:\n\n\n$ ./script/test-travis python2.7\n$ ./script/test-travis pypy\n\n\n\n\n\nThis can help you simulate locally what Travis CI would do. Be aware though that tests may fail depending on your OS, so Travis CI is the reference gate for the project. For instance, installing \nsubprocess32\n in a Pypy environment doesn't work on Mac OSX.", 
            "title": "Contributing"
        }, 
        {
            "location": "/contributing/#contributing-guidelines", 
            "text": "", 
            "title": "Contributing guidelines"
        }, 
        {
            "location": "/contributing/#in-general", 
            "text": "PEP 8 , when sensible.  Test ruthlessly. Write docs for new features.  Even more important than Test-Driven Development,  Human-Driven Development .", 
            "title": "In General"
        }, 
        {
            "location": "/contributing/#in-particular", 
            "text": "Warning  THE WHOLE SECTION IS OUT OF DATE.   Questions, Feature Requests, Bug Reports, and Feedback  ... should all be reported on the  Github Issue Tracker .  Setting Up for Local Development   Fork  simpleflow _ on Github.   Clone your fork::  $ git clone  /botify-labs/simpleflow.git    Make your virtualenv and install dependencies. If you have virtualenv and virtualenvwrapper_, run::  $ mkvirtualenv simpleflow\n$ cd simpleflow\n$ pip install -r requirements-dev.txt    If you don't have virtualenv and virtualenvwrapper, you can install both using  virtualenv-burrito .    Git Branch Structure  simpleflow used to have a separated  devel  branch but is now using only one,\nmain branch  master , that contains what will be released in the next version.\nThis branch is (hopefully) always stable.  Pull Requests    Create a new local branch. ::  $ git checkout -b name-of-feature    Commit your changes. Write  good commit messages .  $ git commit -m \"Detailed commit message\"\n$ git push origin name-of-feature    Before submitting a pull request, check the following:    If the pull request adds functionality, it should be tested and the docs should be updated.    The pull request should work on Python 2.7 and PyPy. Use  tox  to verify that it does.    Submit a pull request to the  master  branch.    Running tests  To run all the tests in your current virtual environment: ::  $ ./script/test  This is what Travis CI does on each environment.  If you want to simulate what Travis CI does, you can approach that by running a container\nfrom them:  $ ./script/test-travis python2.7\n$ ./script/test-travis pypy  This can help you simulate locally what Travis CI would do. Be aware though that tests may fail depending on your OS, so Travis CI is the reference gate for the project. For instance, installing  subprocess32  in a Pypy environment doesn't work on Mac OSX.", 
            "title": "In Particular"
        }, 
        {
            "location": "/license/", 
            "text": "License\n\n\nCopyright 2014 Greg Leclercq\n\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\n\nThe above copyright notice and this permission notice shall be included in\nall copies or substantial portions of the Software.\n\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\nTHE SOFTWARE", 
            "title": "License"
        }, 
        {
            "location": "/license/#license", 
            "text": "Copyright 2014 Greg Leclercq  Permission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:  The above copyright notice and this permission notice shall be included in\nall copies or substantial portions of the Software.  THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\nTHE SOFTWARE", 
            "title": "License"
        }
    ]
}